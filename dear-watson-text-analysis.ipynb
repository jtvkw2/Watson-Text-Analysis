{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Watson Text Analysis"},{"metadata":{},"cell_type":"markdown","source":"## Table of Contents\n\n- [Import Libraries](#import)\n- [Data Cleaning](#data)\n- [Visualization](#visual)\n- [Submission](#submission)"},{"metadata":{},"cell_type":"markdown","source":"<a id =\"import\"></a>\n# Import Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install googletrans","execution_count":2,"outputs":[{"output_type":"stream","text":"Collecting googletrans\n  Downloading googletrans-3.0.0.tar.gz (17 kB)\nCollecting httpx==0.13.3\n  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n\u001b[K     |████████████████████████████████| 55 kB 240 kB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from httpx==0.13.3->googletrans) (2020.6.20)\nCollecting rfc3986<2,>=1.3\n  Downloading rfc3986-1.4.0-py2.py3-none-any.whl (31 kB)\nRequirement already satisfied: chardet==3.* in /opt/conda/lib/python3.7/site-packages (from httpx==0.13.3->googletrans) (3.0.4)\nCollecting httpcore==0.9.*\n  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n\u001b[K     |████████████████████████████████| 42 kB 362 kB/s eta 0:00:01\n\u001b[?25hCollecting sniffio\n  Downloading sniffio-1.1.0-py3-none-any.whl (4.5 kB)\nCollecting hstspreload\n  Downloading hstspreload-2020.8.12-py3-none-any.whl (934 kB)\n\u001b[K     |████████████████████████████████| 934 kB 542 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: idna==2.* in /opt/conda/lib/python3.7/site-packages (from httpx==0.13.3->googletrans) (2.9)\nCollecting h11<0.10,>=0.8\n  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n\u001b[K     |████████████████████████████████| 53 kB 881 kB/s eta 0:00:011\n\u001b[?25hCollecting h2==3.*\n  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n\u001b[K     |████████████████████████████████| 65 kB 1.5 MB/s eta 0:00:011\n\u001b[?25hCollecting hyperframe<6,>=5.2.0\n  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\nCollecting hpack<4,>=3.0\n  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\nBuilding wheels for collected packages: googletrans\n  Building wheel for googletrans (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=15734 sha256=bfea15a1d526ffba015cc9b93c824d33048895d5ea8bbb263652c7ea46a5a6e6\n  Stored in directory: /root/.cache/pip/wheels/20/da/eb/a54579056f265eede0417df537dd56d3df5b9eb2b25df0003d\nSuccessfully built googletrans\nInstalling collected packages: rfc3986, sniffio, h11, hyperframe, hpack, h2, httpcore, hstspreload, httpx, googletrans\nSuccessfully installed googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2020.8.12 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.4.0 sniffio-1.1.0\n\u001b[33mWARNING: You are using pip version 20.2.1; however, version 20.2.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nimport re, string, unicodedata\nfrom pandas import DataFrame\nfrom nltk import word_tokenize, sent_tokenize\nfrom nltk.stem import LancasterStemmer, WordNetLemmatizer\nfrom nltk.probability import FreqDist\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nfrom googletrans import Translator\ntranslator = Translator()\nnltk.download('stopwords')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":3,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n/kaggle/input/contradictory-my-dear-watson/sample_submission.csv\n/kaggle/input/contradictory-my-dear-watson/test.csv\n/kaggle/input/contradictory-my-dear-watson/train.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/contradictory-my-dear-watson/train.csv\")\ntrain.head()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"           id                                            premise  \\\n0  5130fd2cb5  and these comments were considered in formulat...   \n1  5b72532a0b  These are issues that we wrestle with in pract...   \n2  3931fbe82a  Des petites choses comme celles-là font une di...   \n3  5622f0c60b  you know they can't really defend themselves l...   \n4  86aaa48b45  ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...   \n\n                                          hypothesis lang_abv language  label  \n0  The rules developed in the interim were put to...       en  English      0  \n1  Practice groups are not permitted to work on t...       en  English      2  \n2              J'essayais d'accomplir quelque chose.       fr   French      0  \n3  They can't defend themselves because of their ...       en  English      0  \n4    เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร       th     Thai      1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>lang_abv</th>\n      <th>language</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5130fd2cb5</td>\n      <td>and these comments were considered in formulat...</td>\n      <td>The rules developed in the interim were put to...</td>\n      <td>en</td>\n      <td>English</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5b72532a0b</td>\n      <td>These are issues that we wrestle with in pract...</td>\n      <td>Practice groups are not permitted to work on t...</td>\n      <td>en</td>\n      <td>English</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3931fbe82a</td>\n      <td>Des petites choses comme celles-là font une di...</td>\n      <td>J'essayais d'accomplir quelque chose.</td>\n      <td>fr</td>\n      <td>French</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5622f0c60b</td>\n      <td>you know they can't really defend themselves l...</td>\n      <td>They can't defend themselves because of their ...</td>\n      <td>en</td>\n      <td>English</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>86aaa48b45</td>\n      <td>ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...</td>\n      <td>เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร</td>\n      <td>th</td>\n      <td>Thai</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/contradictory-my-dear-watson/test.csv\")\ntest.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"           id                                            premise  \\\n0  c6d58c3f69  بکس، کیسی، راہیل، یسعیاہ، کیلی، کیلی، اور کولم...   \n1  cefcc82292                             هذا هو ما تم نصحنا به.   \n2  e98005252c  et cela est en grande partie dû au fait que le...   \n3  58518c10ba                   与城市及其他公民及社区组织代表就IMA的艺术发展进行对话&amp   \n4  c32b0d16df                              Она все еще была там.   \n\n                                          hypothesis lang_abv language  \n0  کیسی کے لئے کوئی یادگار نہیں ہوگا, کولمین ہائی...       ur     Urdu  \n1  عندما يتم إخبارهم بما يجب عليهم فعله ، فشلت ال...       ar   Arabic  \n2                             Les mères se droguent.       fr   French  \n3                            IMA与其他组织合作，因为它们都依靠共享资金。       zh  Chinese  \n4     Мы думали, что она ушла, однако, она осталась.       ru  Russian  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>lang_abv</th>\n      <th>language</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>c6d58c3f69</td>\n      <td>بکس، کیسی، راہیل، یسعیاہ، کیلی، کیلی، اور کولم...</td>\n      <td>کیسی کے لئے کوئی یادگار نہیں ہوگا, کولمین ہائی...</td>\n      <td>ur</td>\n      <td>Urdu</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cefcc82292</td>\n      <td>هذا هو ما تم نصحنا به.</td>\n      <td>عندما يتم إخبارهم بما يجب عليهم فعله ، فشلت ال...</td>\n      <td>ar</td>\n      <td>Arabic</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>e98005252c</td>\n      <td>et cela est en grande partie dû au fait que le...</td>\n      <td>Les mères se droguent.</td>\n      <td>fr</td>\n      <td>French</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>58518c10ba</td>\n      <td>与城市及其他公民及社区组织代表就IMA的艺术发展进行对话&amp;amp</td>\n      <td>IMA与其他组织合作，因为它们都依靠共享资金。</td>\n      <td>zh</td>\n      <td>Chinese</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>c32b0d16df</td>\n      <td>Она все еще была там.</td>\n      <td>Мы думали, что она ушла, однако, она осталась.</td>\n      <td>ru</td>\n      <td>Russian</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"data\"></a>\n# Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"id            0\npremise       0\nhypothesis    0\nlang_abv      0\nlanguage      0\nlabel         0\ndtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isna().sum()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"id            0\npremise       0\nhypothesis    0\nlang_abv      0\nlanguage      0\ndtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_URL(sample):\n    return re.sub(r\"http\\S+\", \"\", sample)\n\ndef translate(words):\n    translated = translator.translate(words)\n    return translated.text\n\ndef to_lowercase(words):\n    new_words = []\n    for word in words:\n        new_word = word.lower()\n        new_words.append(new_word)\n    return new_words\ndef remove_punctuation(words):\n    new_words = []\n    for word in words:\n        new_word = re.sub(r'[^\\w\\s]', '', word)\n        if new_word != '':\n            new_words.append(new_word)\n    return new_words\n\ndef stem_words(words):\n    stemmer = LancasterStemmer()\n    stems = []\n    for word in words:\n        stem = stemmer.stem(word)\n        stems.append(stem)\n    return stems\n\ndef lemmatize_verbs(words):\n    lemmatizer = WordNetLemmatizer()\n    lemmas = []\n    for word in words:\n        lemma = lemmatizer.lemmatize(word, pos='v')\n        lemmas.append(lemma)\n    return lemmas\n\ndef normalize(words):\n    words = translate(words)\n    words = to_lowercase(words)\n    words = remove_punctuation(words)\n    return words\n\ndef preprocess(sample):\n    return normalize(sample)\n    #nltk.word_tokenize(words)","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocabulary = []\nnew_train = []\nfor text in train['premise']:\n    new_text = preprocess(text)\n    vocabulary.append(new_text)\n    new_train.append(' '.join(new_text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_test = []\nfor text in test['premise']:\n    new_text = preprocess(text)\n    vocabulary.append(new_text)\n    new_test.append(' '.join(new_text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train = DataFrame(new_train,columns=['text'])\nfinal_train['id'] = train['id']\nfinal_train['label'] = train['label']\nfinal_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train['text'].replace('', np.nan, inplace=True)\nfinal_train.dropna(subset=['text'], inplace=True)\nfinal_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_test = DataFrame(new_test,columns=['text'])\nfinal_test['id'] = test['id']\nfinal_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_test['text'].replace('', np.nan, inplace=True)\nfinal_test.dropna(subset=['text'], inplace=True)\nfinal_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"visual\"></a>\n# Visualizations"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens = [item for sublist in vocabulary for item in sublist]\nprint(len(tokens))\nfrequency_dist = nltk.FreqDist(tokens)\nsorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)[0:50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud().generate_from_frequencies(frequency_dist)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"submission\"></a>\n# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = final_train.loc[:12120, 'text'].values\ny_train = final_train.loc[:12120, 'label'].values\nX_test = final_test.loc[:5195, 'text'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\ntrain_vectors = vectorizer.fit_transform(X_train)\ntest_vectors = vectorizer.transform(X_test)\nprint(train_vectors.shape, test_vectors.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB().fit(train_vectors, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = clf.predict(test_vectors)\noutput = pd.DataFrame({'id': final_test.id, 'prediction': predicted})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}